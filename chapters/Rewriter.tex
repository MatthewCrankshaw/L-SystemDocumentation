
\lettrine[lines=3]{T}{}here are two major parts necessary to procedurally generate plant-life using an L-system. These are the rewriter and the interpreter. The purpose of the L-system rewriter is to take an L-system file as input, and generate the resulting string that fits the L-system grammar. It does this by syntactically and semantically analysing the L-system input, and generating the structures and information necessary to carry out the rewriting process. The rewriting process uses the structures and information, such as the string of modules and the production rules, to step through each string and rewrite the symbols. This chapter focuses on each part of the string rewriters' implementation and will introduce a technique of processing the L-systems' input, similar to how computer languages are compiled. This chapter will also formally define the L-system grammar in Backus-Naur Form, and provide the pseudocode for the L-system rewriter. 

For a simple D0L-system, like the one seen in section \ref{DOL-system example}. Each symbol within the alphabet is made up of a single letter, the productions rules then match against each letter in a string. As the D0L-system is deterministic, there is no randomness when determining the matching rule. The simplicity of the D0L-system makes it quite easy to create a rewriting system for it. All the rewriter must do is store the starting string and production rule predecessors and successors. It then iterates over a string of symbols and replace them with the successor. The implementation of a more sophisticated L-system, like the parametric 0L-system, is much more complex. A parametric L-system can have multiple modules that make up a string, where each module may have multiple parameters, and each parameter could be a mathematical expression. The added complexity makes developing a rewriting system considerably more difficult. The rewriter must better understand what the syntax of the L-system is specifying, based on the context of each symbol within the L-system.

Due to the complexity of the L-system grammar, it is difficult for a computer to tell the syntactic and semantic properties of each part of the L-system input, further increasing the complexity of the rewriting process. Using a system similar to a computer language \say{compiler}, an L-system "program" can be broken down into a three-stage process, as seen in figure \ref{rewriting system} below. The first stage is \textit{lexical analysis}, then a process called \textit{parsing} and finally the string rewriting stage. The lexical analyser is responsible for splitting the input into syntactic words, and then assigning each word into its syntactic category. Any word within the L-system that does match a syntactic category will result in a lexical error. If there are no lexical errors the words and their syntactic categories are sent to the parser. The parser matches the syntactical categories of each sentence in the language against a grammatical model. If any of the sentences within the language do not match the grammatical model, an appropriate error message can be displayed, similar to that of the lexical error. The error states where the syntax error occurred and what was grammatically incorrect. The parser also creates a syntax tree along with any data structures necessary for the rewriting process. These structures can then be used to carry out string rewriting or provide information to the interpreter.

\begin{figure}[htbp]
	{\centering
		\setlength{\fboxrule}{1pt}
		\vspace{7px}
		\fbox{
			\includegraphics[scale=0.32]{Diagrams/StringRewriter.png}
		}
		\caption{Diagram showing the parts of the rewiting system.} \label{rewriting system}
	}
\end{figure}
\FloatBarrier

\section{Environment and Tools}

The implementation of the string rewriter, and the string interpreter, is written in the C and C++ programming languages \cite{stroustrup2000c++}. The C and C++ languages are two of the most common programming languages that have stood the test of time with the first version of C being released in 1974. These languages are frequently used within computer graphics, with some of the most popular game engines supporting either C or C++. Such as CryEngine, Unreal Engine, Source Engine, and more. The main reason for this is the high performance and low-level memory management that C and C++ provide, and the graphics programming frameworks such as OpenGL, Vulkan, and DirectX all having direct support for either C or C++. The C and C++ languages also have a large number of useful libraries that provide extra functionality.  

The implementation of the rewriter and the interpreter will use the modern Open Graphics Library (\gls{OpenGL}). The OpenGL framework is one of the industry standards for creating 3D graphics applications. It is a cross-platform API for interacting with the \acrshort{gpu} in a low-level way. The high-performance nature of OpenGL is essential, as displaying and simulating the L-system can be very graphically intensive \cite{sellers2013opengl} \cite{movania2017opengl}. OpenGL was initially intended to be an \acrshort{api} for the C and C++ programming languages. Therefore, both the programming language and graphics API have a strong emphasis on performance, which is necessary when procedurally generating and simulating plant-life.

For more specialised mathematics capabilities, the \acrfull{glm} library holds many mathematics classes and functions for conveniently dealing with structures such as vectors, matrices, and quaternions. This thesis will cover these mathematical concepts in chapter \label{maths chapter}; however, it is convenient to have these implemented and tested within a C++ library. Another important library is \acrfull{glfw} which is a multi-platform \acrshort{api} for creating an managing user interface windows, events, and user-input \cite{glfwDocumentation}. To keep track of changes and manage versions. Git is a free and open-source version control software. It can keep track of changes that have been made to the files within a project folder as well as keep previous versions of the project throughout the development process. In conjunction with Git,  Github is an online web application that stores git repositories. Git acts as a backup as well as containing all previous versions of the project \cite{torvalds}.

\section{The L-system as an Interpreted Grammar}

Traditionally an interpreter in computing is a program that takes program code as input. It is then analyzed and interpreted as it is encountered in the execution process. All of the previously encountered information is kept for later interpretations. The information about the program can be extracted by inspecting the program, such as the set of declared variables in a block or a function \cite{wilhelm2010compiler}. In essence, the L-system rewriter contains a type of interpreter. This should not be confused with the interpreter that processes the resultant string using turtle graphics. Due to this confusion of terms, the system containing the lexical analyser, L-system parser, and the string rewriter will be referred to as the L-system rewriter, instead of the interpreter in the computational sense. 

A similarity can be drawn between traditionally interpreted languages and the L-system rewriter. The L-system rewriter defines a set of constant variables, a starting point, and then some production rules. This information can then be used to rewrite the starting string several times. Later on, it may be decided that, instead of five generations of rewriting, the rewriter should instead generate ten. Some information about the L-system is still valid, the production rules, axiom, and constants have not changed, and therefore this information can be used to interpret to the tenth generation. This concept can be used to go from the current state of the L-system rewriter and rewrite another five times. Instead of throwing all the information away and starting from scratch. Furthermore, if we would like to retrieve the resultant string, this can be requested from the L-system rewriter. 

The lexical analyser and parser are a necessary part to carry out rewriting. Without the lexical analyser or parser, it would not be straightforward to find the syntactic roles of each part of the L-system. Take the example of the module: F(2*3, x * (2 + y)). Here there is a single module with two parameters, one parameter has the expression (2 * 3), and the other has the expression (x * (2+y)). These complex structures within a grammar require knowledge about the grammatical model it represents. The lexical analyser firstly makes sure that all the syntax within the L-system is correct and assigns each word or symbol to a syntactic category, the parser then splits the L-system into its components and is describes each parts syntactic roll. The lexical analyser provides the understanding that x and y are variables within a module and do not represent something else. It also provides knowledge about how to find the values of x and y. 

The difficulty of creating an L-system with more complexity in the grammar is that it becomes more challenging to write a valid L-system to represent a particular structure. For example, imagine trying to write a C program where the compiler does specify why the program is incorrect. The advantage of using a rewriter similar to a compiler is that it makes it simpler to debug any syntactic errors, as well as make the string rewriting much faster. This means that writing an L-system becomes similar to rewriting a recursive program, where any syntactic mistakes will result in a meaningful error describing what was incorrect.

\section{The Syntax of a Parametric L-system}

This section will specify the valid syntax for the parametric L-system rewriter. The syntax is similar to the definition of the parametric L-system definition given by Prusinkiewicz and Lindenmayer in section \ref{definition of a parametric 0L-system section}. There are some additions and modifications to the syntax definition provided by Prusinkiewicz and Lindenmayer to construct an L-system that includes branching, constant variable definitions, object specifications, parametric L-system concepts, randomness, and stochastic L-systems \cite{prusinkiewicz2012algorithmic}. 

This L-system has five major parts. Each part is categorised as a statement. Valid statements are the \textit{defines} , the \textit{includes}, a single generation statement, a single axiom statement, and one or more production rules \cite{prusinkiewicz2013lindenmayer}. All of these statements collectively form an L-system. Each statement starts with a `\#' character and ends with a `;' symbol. These are used to indicate the start and end of a statement, even if multiple statements are written on the same line. 

\noindent
The order that statements should be listed is as follows: 

\begin{equation} \label{statement order example}
\begin{aligned}
	&\text{\#generations statement;}\\
	&\text{\#define statements;}\\
	&\text{\hspace{10mm}...}\\
	&\text{\#include statements;}\\
	&\text{\hspace{10mm}...}\\
	&\text{\#axiom statement;}\\
	&\text{\#production statements;}\\
	&\text{\hspace{10mm}...}\\
\end{aligned}
\end{equation}

The order for the statements does not always matter; for instance, the generation statement can be defined anywhere within the L-system. However, some parts are required to be in a particular order, such as the define and include statements, which must appear above the axiom and production rule statements as they define values used within the axiom and production rules. It is best practice to specify the L-system in the above order as to avoid any conflictions or errors.

All numbers within the L-system are represented as floating-point numbers. Using a single data-type keeps all numbers consistent. Other data types could be added in the future; however, there are added complexities in doing so, such as the conversion from one type to another, or having to specify which data type a variable represents. The floating-point data type provides all the necessary functionality needed for the L-system; therefore, it seems unnecessary to add more data types. 

\section{The L-system Lexical Analyser} \label{Flex}

In computer science, specifically the study of programming language compilers, the program responsible for carrying out lexical analysis is the lexer. Depending on the literature the lexer can also be known as the tokenizer or scanner. D. Cooper and L. Torczon write that ``The scanner, or lexical analyser, reads a stream of characters and produces a stream of words. It aggregates characters to form words and applies a set of rules to determine whether each word is legal in the source language. If the word is valid, the scanner assigns it a syntactic category or part of speech'' \cite{cooper2011engineering}. This is no different for the parametric 0L-system rewriter. For the rewriter to have enough information to carry out rewriting, it must first understand what each word or token within the L-system means, this requires assigning a syntactic category to each token, and whether or not the token is valid or not within the L-system grammar.  

The scanner itself is quite complex, its main goal is to match the characters or strings within the language, to either a word or a regular expression defined in the grammar. When the match is made the token is given a syntactic category. The mechanism by which it achieves this is known as \textit{finite automata} \cite{wilhelm2013compiler}. It is possible to write custom \gls{Lexer}, however, it can be quite complicated and time-consuming to design and implement, and once a custom \gls{Lexer} has been created it is also difficult to change functionality at a later stage. There is a well known program known as the \acrlong{flex} (\acrshort{flex}). \acrshort{flex} takes in a file which contains the lexical rules of the language, this being the strings as well as the regular expression as well as its associated syntactic category. When \acrshort{flex} is executed it will create a \gls{Lexer} in the form of a C program. To create a lexer with \acrshort{flex}, the lexical rules must be defined. Below are the characters, strings and regular expressions and their associated syntactic categories, as well as a description as to its use in the parametric 0L-system. 

\begin{table}[h!] \center
\begin{tabular}{ | c | l | l |}
\hline
	Syntactic Word	& Syntactic Category & Description\\  
\hline
\hline
	, 				& T\_COMMA 				& Separation between module parameters \\
\hline
	: 				& T\_COLON 				& Separation between production rule parts \\
\hline
	; 				& T\_SEMI\_COLON 		& End of a statement\\
\hline
	\#				& T\_HASH 				& Beginning of a statement\\
\hline
	( 				& T\_PARENL 			& Start of a modules parameters \\
					&						& or specifies presidence in an expression \\
\hline
	) 				& T\_PARENR 			& End of a modules parameters \\
					&						& or specifies presidence in an expression \\
\hline
	\{ 				& T\_BRACKETL 			& Start of a random range\\
\hline
	\} 				& T\_BRACKETR 			& End of a random range\\
\hline
	$\sim$ 			& T\_TILDE 				& Stochastic operator\\
\hline
	$==$				& T\_EQUAL\_TO 			& Relational operator stating equal to\\
\hline
	$!=$				& T\_NOT\_EQUAL\_TO 	& Relational operator for not equal to\\
\hline	
	$<$ 			& T\_LESS\_THAN 		& Relational operator for less than\\
\hline
	$>$ 			& T\_GREATER\_THAN 		& Relational operator for greater than\\
\hline
	$<=$ 			& T\_LESS\_EQUAL 		& Relational operator for greater or equal\\
\hline
	$>=$  			& T\_GREATER\_EQUAL 	& Relational operator for greater or equal\\
\hline
	[ 				& T\_SQUARE\_BRACEL 	& Module name (branching save state) \\
\hline
	] 				& T\_SQUARE\_BRACER 	& Module name (branching load state) \\
\hline
	+ 				& T\_PLUS 				& Arithmetic operator for addition, or\\
					&						& Module name (Yaw right) \\
\hline
	- 				& T\_MINUS 				& Arithmetic operator for subtraction, or\\
					&						& Module name (Yaw left)\\
\hline
	/ 				& T\_FORWARD\_SLASH 	& Arithmetic operator for division, or\\
					&						& Module name (Pitch up)\\
\hline
	$\backslash$ 		& T\_BACK\_SLASH 		& Module name (Pitch down)\\
\hline
	* 				& T\_STAR 				& Arithmetic operator for multiplication, or\\
					&						& Condition in a production rule which is true\\
\hline
	$\land$		& T\_HAT 				& Arithmetic operator for and exponent, or\\
					&						& Module name (Roll right)\\
\hline
	$\&$ 			& T\_AMPERSAND 			& Module name (Roll left)\\
\hline
	! 				& T\_EXCLAMATION 		& Module name (Set size of branch)\\
\hline
	\$ 				& T\_DOLLAR 			& Module name \\
\hline
	= 				& T\_ASSIGN 			& Assignment operator used to set generations\\
\hline
	\#n 			& T\_GENERATIONS 		& Declaration of the number of generations\\
\hline
	\#w 			& T\_AXIOM 				& Declaration of the axiom\\
\hline
	\#define 			& T\_DEFINE 		& Declaration of the define\\
\hline
	\#object 			& T\_OBJECT			& Declaration of the object\\
\hline
	[0-9]+.[0-9]+$|$[0-9]+ 					& T\_FLOAT 				& Regular expression of floating point number\\
\hline
	[a-zA-Z\_][a-zA-Z0-9\_]*  				& T\_VAR\_NAME 			& Regular expression of module or variable name\\
\hline
\end{tabular}
\caption{Table of valid lexer words}
\label{lexer words}
\end{table}
\FloatBarrier

\noindent
From the table above, several syntactic categories contain more than one meaning; for instance, the open and close parentheses have two meanings. They are used to either specify a modules' parameters or to specify precedence within an expression. It is not up to the scanner to determine what each parenthesis means, or that it has a meaning at all, the lexer only recognises that it falls into the syntactic categories, T\_PARENL and T\_PARENR. Deriving the meaning of a given token or syntactic category is decided by the parser. The parser is more aware of the context of each syntactic word. Similarly, the symbols [,],+,-,/,$\backslash$, $\land$, $\&$, !, \$, and T\_VAR\_NAME are valid module names. These symbols need to be specifically defined as their syntactic category, as they not only represent a module name but can also represent a different meaning depending on their context. For instance, the +, -, / are valid module names, but they also are mathematical symbols used within arithmetic expressions. The scanner must separate these symbols and keep them in their syntactic category for the parser to be able to understand the same symbol in multiple contexts. 

It is also important to note that there are two unique types of tokens. These are the T\_FLOAT and T\_VAR\_NAME. The regular expression for T\_FLOAT will match any floating-point value, and the regular expression for T\_VAR\_NAME will match with any valid variable name. These unique tokens are valid syntactic categories but also contain an associated value. For instance, T\_FLOAT has a floating-point value associated with it, and T\_VAR\_NAME has a string value associated with it. These values must be kept and provided to the parser for use later on.


\section{The L-system Parser} \label{parser}

The parsers' job is to find out if the input stream of words from the \gls{Lexer} is a valid sentence according to the grammar. If the syntactical categories from the \gls{Lexer} match the grammatical model, then the syntax is seen to be correct. If the syntax of the language is correct, the \gls{Parser} will generate a syntax tree and build the relevant data structures for use later on in the compilation process \cite{cooper2011engineering}. For the L-system rewriter, the syntax tree and data structures are not used for compilation but rather for the string rewriting process. 
 

In order to describe a grammar, a suitable notation is necessary to express its syntactic structure and grammatical model. According to Cooper, the \acrlong{bnf}(\acrshort{bnf}) has traditionally been used by computer scientists to represent context-free grammars such as programming languages. Its origins are from the late 1950s and early 1960s. The \acrshort{bnf} notation represents the context-free grammar by defining a set of non-terminal symbols that derive from a set of terminal or non-terminal symbols. Terminal symbols are elementary symbols of the language defined by the formal grammar. A terminal symbol will eventually appear in the resulting formal language. On the other hand, a non-terminal symbol exists only as a placeholder for patterns of terminal symbols but does not appear within the formal language itself. The syntactic convention for a \acrshort{bnf} is for non-terminal symbols to be surrounded by angled brackets. For instance, $<$expression$>$ and terminal symbols, such as the symbol for addition \say{+} to be underlined, but nowadays, it is not often underlined. The symbol $\epsilon$ represents an empty string, the ::= means \say{derives} and the $\mid$ means \say{also derives} but is often articulated as an \say{or} \cite{cooper2011engineering}. The very first derivation must be a non-terminal symbol called the goal symbol. The goal symbol is a set of all valid derived strings. This means that the goal symbol is not a word within the language, but rather a syntactic variable in the form of a non-terminal symbol. The \acrshort{bnf} notation below can be used to represent a simple grammar for arithmetic expressions, where the terminal \say{number} is any valid integer, and the goal symbol is $<$expression$>$. Below is the \acrshort{bnf} notation for the syntax of an arithmetic expression that can represent addition and subtraction.

\noindent
\begin{algorithm}
\begin{bnf*}
	\bnfprod{expression}
		{\bnfts{number}}\\
		\bnfmore{\bnfor \bnfts{(} \bnfpn{expression} \bnfts{)}}\\
		\bnfmore{\bnfor \bnfpn{expression} \bnfts{+} \bnfpn{expression}}\\
		\bnfmore{\bnfor \bnfpn{expression} \bnfts{-} \bnfpn{expression}}
\end{bnf*}
\end{algorithm}
\FloatBarrier

\noindent
The \acrshort{bnf} above states that the goal symbol, $<$expression$>$ derives from one of four states. Either a terminal number, or an expression contained within two parentheses, or two expressions either side of an addition or subtraction terminal symbol. This type of notation is recursive and allows the formal language to write expressions that exist within other expressions. For example the expression \say{5 + 10 - (20 + 2)} can be broken down into using the \acrshort{bnf} production rule forming a syntax tree as seen in figure \ref{syntax tree} below. In this case, the whole expression fits the grammatical model of the language. Thus it can be parsed, forming the syntax tree. Computationally, when parsed, this expression will create a data structure, which will be discussed in more detail in section \ref{expression tree}.

\begin{figure}[htbp]
	{\centering
		\vspace{7px}
		
		\includegraphics[scale=0.7]{Diagrams/syntaxTree.png}
		
		\caption{Diagram of the syntax tree for an expression.} \label{syntax tree}
	}
\end{figure}
\FloatBarrier

\noindent
Similar to the scanner, the parser program can be quite complex. It needs to find the associated terminal and non-terminal symbols and comply with the grammatical model. Furthermore, if there is a change in the grammar or there is a need to add features at a later date, it is frequently difficult to change the parser. Many studies have been conducted on creating a parsers; however this is beyond the scope of this thesis. Therefore, a program called a parser generator can be used to create the parser program. It uses a specification of the grammar similar to that of the \acrshort{bnf} to generate a C program capable of parsing a given language. A popular implementation of a parser generator is called Bison.

\subsection{\acrlong{bnf} of the L-system Grammar} \label{L-system Grammar}

A \acrshort{bnf} below is used to describe any possible valid L-system. The Bison program takes a definition similar to this one and creates the parser program. The parser takes in an L-system as input and will process and output the appropriate data structures and information necessary to carry out rewriting.

\begin{singlespace}
	\begin{bnf*}
	\bnfprod{lSystem}
		{\bnfes \bnfor \bnfpn{statements} \bnfsp \bnfts{EOF}}\\
	\bnfprod{statements}
		{\bnfes \bnfor \bnfpn{statement} \bnfpn{statements}}\\
	\bnfprod{statement}
		{\bnfts{EOL} \bnfor \bnfpn{generation} 
		\bnfor \bnfpn{definition} 
		\bnfor \bnfpn{object} 
		\bnfor \bnfpn{axiom} 
		\bnfor \bnfpn{production}}\\
	\bnfprod{generation}
		{\bnfts{\#define} \bnfsp \bnfts{=} \bnfsp \bnfpn{float} \bnfts{;}}\\
	\bnfprod{float}
		{\bnfts{[0-9]+.[0-9]+$|$[0-9]+}}\\
	\bnfprod{variable}
		{\bnfts{[a-zA-Z\_][a-zA-Z0-9\_]*}}\\
	\bnfprod{number}
		{\bnfpn{float} \bnfor \bnfts{-} \bnfsp \bnfpn{float}}\\
	\bnfprod{range}
		{\bnfts{\{} \bnfpn{number} \bnfts{,} \bnfpn{number} \bnfts{\}}}\\
	\bnfprod{definition}
		{\bnfts{\#define} \bnfsp \bnfpn{variable} \bnfsp \bnfpn{number} \bnfts{;}}\\
	\bnfprod{object}
		{\bnfts{\#object} \bnfsp \bnfpn{variable} \bnfsp \bnfpn{variable} \bnfts{;}}\\ 
	\bnfprod{module}
		{\bnfpn{variable} \bnfor \bnfts{+} 
		\bnfor \bnfts{-} 
		\bnfor \bnfts{/} 
		\bnfor \bnfts{$\backslash$} 
		\bnfor \bnfts{$\land$}
		\bnfor \bnfts{\&}
		\bnfor \bnfts{\$}
		\bnfor \bnfts{[}
		\bnfor \bnfts{]}
		\bnfor \bnfts{!}}\\
		\bnfmore{\bnfor \bnfts{+} \bnfts{(} \bnfpn{param} \bnfts{,} \bnfsp \bnfpn{paramList} \bnfts{)}}\\
		\bnfmore{\bnfor \bnfts{-} \bnfts{(} \bnfpn{param} \bnfts{,} \bnfsp \bnfpn{paramList} \bnfts{)}}\\
		\bnfmore{\bnfor \bnfts{/} \bnfts{(} \bnfpn{param} \bnfts{,} \bnfsp \bnfpn{paramList} \bnfts{)}}\\
		\bnfmore{\bnfor \bnfts{$\backslash$} \bnfts{(} \bnfpn{param} \bnfts{,} \bnfsp \bnfpn{paramList} \bnfts{)}}\\
		\bnfmore{\bnfor \bnfts{$\land$} \bnfts{(} \bnfpn{param} \bnfts{,} \bnfsp \bnfpn{paramList} \bnfts{)}}\\
		\bnfmore{\bnfor \bnfts{\&} \bnfts{(} \bnfpn{param} \bnfts{,} \bnfsp \bnfpn{paramList} \bnfts{)}}\\
		\bnfmore{\bnfor \bnfts{\$} \bnfts{(} \bnfpn{param} \bnfts{,} \bnfsp \bnfpn{paramList} \bnfts{)}}\\
		\bnfmore{\bnfor \bnfts{[} \bnfts{(} \bnfpn{param} \bnfts{,} \bnfsp \bnfpn{paramList} \bnfts{)}}\\
		\bnfmore{\bnfor \bnfts{]} \bnfts{(} \bnfpn{param} \bnfts{,} \bnfsp \bnfpn{paramList} \bnfts{)}}\\
		\bnfmore{\bnfor \bnfts{!} \bnfts{(} \bnfpn{param} \bnfts{,} \bnfsp \bnfpn{paramList} \bnfts{)}}\\
	\bnfprod{axiom}
		{\bnfts{\#w} \bnfsp \bnfts{:} \bnfsp \bnfpn{axiomStatementList} \bnfts{;}}\\
	\bnfprod{axiomStatementList}
		{\bnfes \bnfor \bnfpn{axiomStatement} \bnfpn{axiomStatementList}}\\
	\bnfprod{axiomStatement}
		{\bnfpn{module}}\\
	\bnfprod{paramList}
		{\bnfes \bnfor \bnfpn{param} \bnfpn{paramList}}\\
	\bnfprod{param}
		{\bnfpn{expression}}\\
	\bnfprod{expression}
		{\bnfpn{variable} \bnfor \bnfpn{number} \bnfor \bnfpn{range}}\\
		\bnfmore{\bnfor \bnfpn{expression} \bnfts{+} \bnfpn{expression}}\\
		\bnfmore{\bnfor \bnfpn{expression} \bnfts{-} \bnfpn{expression}}\\
		\bnfmore{\bnfor \bnfpn{expression} \bnfts{*} \bnfpn{expression}}\\
		\bnfmore{\bnfor \bnfpn{expression} \bnfts{/} \bnfpn{expression}}\\
		\bnfmore{\bnfor \bnfpn{expression} \bnfts{$\land$} \bnfpn{expression}}\\
		\bnfmore{\bnfor \bnfts{(} \bnfpn{expression} \bnfts{)}}\\
	\bnfprod{production}
		{\bnfts{\#} \bnfpn{variable} \bnfsp \bnfts{:} \bnfsp \bnfpn{predecessor} \bnfsp \bnfts{:} \bnfsp \bnfpn{condition} \bnfsp \bnfts{:} \bnfsp \bnfpn{successor} \bnfts{;}}\\
		\bnfprod{predecessor}
		{\bnfpn{predecessorStatementList}}\\
	\bnfprod{predecessorStatementList}
		{\bnfes \bnfor \bnfpn{predecessorStatement} \bnfpn{predecessorStatementList} }\\
	\bnfprod{predecessorStatement}
		{\bnfpn{module}}\\
	\bnfprod{condition}
		{\bnfts{*}}\\
		\bnfmore{\bnfor \bnfts{$\sim$} \bnfpn{float}}\\
		\bnfmore{\bnfor \bnfpn{leftExpression} \bnfpn{operator} \bnfpn{rightExpression}}\\
	\bnfprod{leftExpression}
		{\bnfpn{expression}}\\
	\bnfprod{rightExpression}
		{\bnfpn{expression}}\\
	\bnfprod{operator}
		{\bnfts{==} \bnfor \bnfts{!=} \bnfor \bnfts{$<=$} \bnfor \bnfts{$>=$} \bnfor \bnfts{$>$} \bnfor \bnfts{$<$}}\\
	\bnfprod{successor}
		{\bnfpn{successorStatementList}}\\
	\bnfprod{successorStatementList}
		{\bnfes \bnfor \bnfpn{successorStatement} \bnfpn{successorStatementList}}\\
	\bnfprod{successorStatement}
		{\bnfpn{module}}\\
	\end{bnf*}

\newpage

\end{singlespace} 

\noindent
As seen above in the \acrshort{bnf} notation for a L-system, the goal state is $<$lSystem$>$. The $<$lSystem$>$ can be made up of $<$statements$>$ beginning with the symbol \say{\#} and ending with the symbol \say{;}, or the \acrlong{eof} (\acrshort{eof}) character signifying the end of the L-system. Each non-terminal $<$statements$>$ is made up of a $<$statement$>$ followed by more $<$statements$>$, or an empty string ($\epsilon$). The $<$statement$>$ itself can either be an \acrlong{eol} (\acrshort{eol}) character or a $<$generation$>$, $<$definition$>$, $<$object$>$, $<$axiom$>$ or $<$production$>$ statement. The non-terminal symbols $<$float$>$ and $<$variable$>$ specify a regular expression. Each statement then has a number of terminal and non-terminal derivatives that allow the production of all valid L-systems that follow this grammar. 

In the previous chapter, the scanner defined the syntactic categories. These syntactic categories are all the valid terminal symbols within the L-system grammar. In essence, the parser takes these syntactic categories and finds if they fit the above \acrshort{bnf}, and if so, it extracts the information from the L-system and generates the relevant data structures and syntax tree. 

\subsection{Dealing with Constant Values and Objects} \label{constants subsection}

Defining constants and objects is essential as it allows the specification of named variables and module names that have a particular meaning. To define a constant or an object is syntactically similar. The keyword define or include is used, then a variable name followed by a value. The value for a constant is a floating-point number, and the value for an \textit{include} is a name of an object within the predefined object library. Seen below is an example of defining a constant and an object: 

\begin{equation} \label{constant and object example}
\begin{aligned}
	&\text{\#define num 10;}\\
	&\text{\#define pi 3.1415;}\\
	&\\
	&\text{\#include F BRANCH;}\\
	&\text{\#include S SPHERE;}\\
\end{aligned}
\end{equation}

\noindent
The definition variables can be stored as a table, called a constants table, which keeps track of all of the constant variable names as well as their values defined by the L-system, as seen in the table below: 

\begin{table}[h!] \center
\begin{tabular}{ | c | l | }
\hline
	Variable Name 	& Value\\  
\hline
\hline
	num 				& 10.0\\
\hline
	pi					& 3.1415\\
\hline
\end{tabular}
\caption{Variable table for storing constants}
\label{constants table}
\end{table}
\FloatBarrier

\noindent
The object table structure is very similar to the constants table. The object table holds the module name, and name of the object in the predefined object library. The object table is not used during rewriting, but it is necessary to provide information during the interpretation of the resulting string. 

\begin{table}[h!] \center
\begin{tabular}{ | c | l | }
\hline
	Module Name	& Object Name\\  
\hline
\hline
	F 				& BRANCH\\
\hline
	S				& SPHERE\\
\hline
\end{tabular}
\caption{Object table for storing modules and their associated object}
\label{constants table}
\end{table}
\FloatBarrier

\subsection{Implementing Modules and Strings} \label{modules and strings}

For the rewriter, it is crucial to understand that there are three significant parts of a module. There is a module name, which is a symbol or string of symbols. Secondly, there is a list of zero or more parameters signified by the open and close parenthesis. If there are no parameters for a module, it can be specified without parenthesis. However, if there are no parameters, there should then be a space between the current module and the next module. Thirdly, each parameter can either contain a number, variable, random number range, or a mathematical expression containing numbers, variables, and parentheses signifying precedence. 

It is important to note that there are two types of modules. One being a module definition and the other a module call. Although these are two different types of modules, they can refer to the same thing. The module definition stands as a template for a module within a production rule. These templates do not have to hold actual values but rather the variable names or random ranges, which will be substituted during the rewriting process. Module calls, on the other hand, would appear either in the axiom or in the resultant string. The parameters of a module call will always hold actual numerical values. Below is an example outlining the difference between the module definition and module calls.

\begin{equation} \label{module definition and call example}
\begin{aligned}
	&\text{\#w : A(10, 20);}\\
	&\text{\#p1 : A(x, y) : * : A(x+y, y); }\\
\end{aligned}
\end{equation}

\noindent
In the example \ref{module definition and call example} above, module A(10, 20) within the axiom is a module call, as it contains two numerical values of 10 and 20. In the production rule p1, the predecessor is the module A(x, y), this is a module definition, it states that module A's first parameter has a local variable x, and its second parameter has the local variable y. The calling modules values 10 and 20 will substitute x and y anywhere within the successor statement. The production rule p1's successor has a single module A(x+y, y). This is also a module definition; however, the variables will be substituted during rewriting with the calling modules value. When substituted, the successor will be A(10+20, 20). This module can be further evaluated to A(30, 20). After the successor module has been substituted and evaluated, the successors' modules must have a numerical value. They then become module calls within the resultant string ready for the next stage of rewriting.

A string in the context of a parametric L-system is a list of modules. The modules are linked one after the other, creating a type of string.

\subsection{Implementing Arithmetic Expressions Trees} \label{expression tree}

As stated previously within the L-system \acrshort{bnf}, an expression is either a variable name, a number, or a random range. It is also possible that an expression is part of another expression. Take the example: $5 \times 4 + n$, here there are three expressions 5, 4 and $n$ however, $5 \times 4$ is also an expression, as well as $4 + n$. An expression can also be described as any of the expressions above between a set of parentheses, such as $(4+n)$. The result of the expression is calculated from left to right unless parentheses are used, which prioritises the encapsulated expression to be calculated first. We can represent this expression as an expression tree in the diagram below:


\begin{figure}[htbp]
	{\centering
		\setlength{\fboxrule}{1pt}
		\vspace{7px}
		\fbox{
			\includegraphics[scale=0.4]{Diagrams/ThesisContent.png}
		}
		\caption{Diagram of an expression tree.} \label{3D rotations}
	}
\end{figure}
\FloatBarrier

\noindent
The parser provides a syntax tree, which makes it easy to generate the above expression tree. The expression tree can be made up od four types of nodes: a variable, number, random range, or an operator. The leaf nodes of the expression tree must be either a number, variable or random range; moreover, a connecting node within the tree must be an operator. We can then traverse the generated tree and replace the variables with their associated value. For random ranges, the random value can be generated and assigned to the node. A second traversal during the rewriting process can then computes the result of the expression.

\subsection{Implementing Random Ranges} \label{random range}

L-systems are limited in the amount of variation they produce during the rewriting stage. In nature, the variation between the two plants depends on an enormous number of factors. These factors ultimately create variation within the branching structure and in the features of the branches, leaves, and flowers. These features include but are not limited to, branching angles, width, length, height, and weight. When introducing variation in the L-system branching structure, there must be randomness in how rules are chosen. This topic is discussed in section \ref{stochastic rules}. However, this section introduces a method of providing variation in the features of branch segments, called random ranges.

A random range is a method of declaring a variable that represents a number that is randomly generated between two bounding numbers. The bounding numbers are the minimum and a maximum, respectively. The primary method used for generating a pseudo-random number using a uniform distribution within a range can be seen below. 

\begin{singlespace}
\begin{algorithm}
\begin{algorithmic}[1]
\Procedure{Random Range}{min, max}
	\State n $\gets$ (rand() \% (max - min + 1)) + min
	\State \textbf{return} n
\EndProcedure
\end{algorithmic}
\end{algorithm}
\end{singlespace}

\newpage
Several other types of pseudo-random number generators could generate numbers according to different distributions, such as normal, binomial, Poisson, among others. When generating plant-life, a uniform distribution should be sufficient for most features and plant-life.

A random range can be declared in three different places within the L-system. It can be declared in the define statement, as an axiom parameter, or a production rule successor parameter. If the random range is declared within a define statement or the axiom, it will generate the random value during the parsing stage. However, if the range is defined in the successor, the number is generated during the rewriting process. More specifically, it is generated when the expressions within the successors are being evaluated. The values are generated during the rewriting process, rather than during parsing because each time a module is rewritten, the number should be a new random number. Generating the numbers during parsing means that the random number is only generated once, and then kept for use later. Conversely, generating the number during rewriting means that a new number will be generated every time rewriting takes place. 

\subsection{Implementing Stochastic Rules} \label{stochastic rules}

The term \say{stochastic} refers to a randomly determined process. This could be by a uniform distribution or some random probability distribution. 

One of the important factors of generating plant-life is being able to simulate randomness in the generation process. Section \ref{random range} covers a method of generating random numbers that can be used for the features within an L-system. This section covers a different type of randomness that affects the way the rewriter selects a rule for rewriting. In this way, rules can be selected randomly instead of meeting certain conditions. Randomly selecting rules provides randomness within the structure of the plant-life rather than the features.

In order to achieve stochastic rules, each rule must belong belonging to a stochastic group of rules that provides a probability value. The probability indicates how likely it is that rule is selected during the rewriting process. For production rules to be part of the same stochastic group, they are required to meet the following four criteria: 

\begin{itemize}
\item The stochastic operator $\sim$ must be used with a probability between 0.0 and 1.0.
\item The predecessor module name must match the other predecessor module names within that stochastic group.
\item The number of parameters within the predecessor must match the number of parameters of other production rules within that stochastic group.
\item The total probability of all of the production rules within the stochastic group must not exceed 1.0 or be less than 0.0.
\end{itemize}

\noindent
During the parsing phase, if the rule has the stochastic operator, the probability of the rule must be kept for later use within a stochastic probability table. The table also keeps track of which rules are associated with which stochastic groups. A stochastic probability table can be generated from the rules below, as seen in table \ref{stochastic table}. 

\begin{equation} \label{stochastic implementation example}
\begin{aligned}
	&p_1~ :  F(x)~ :~ \sim 0.33 ~ :~ F(x)[+(r)F(x)]F(x)[-(r)F(x)]F(x)\\
	&p_2~ :  F(x)~ :~ \sim 0.33 ~ :~ F(x)[+(r)F(x)]F(x)\\
	&p_3~ :  F(x)~ :~ \sim 0.34 ~ :~ F(x)[-(r)F(x)]F(x)\\
\end{aligned}
\end{equation}

\begin{table}[h!] \center
\begin{tabular}{ | c | c | c | }
\hline
	Stochastic Group & Rule Name & Probability\\  
\hline
\hline
\multirow{3}{*}{F1} & p1 & 0.33 \\
& p2 & 0.33 \\
& p3 & 0.34 \\
\hline
\end{tabular}
\caption{Stochastic rule table for holding rule probabilities within a stochastic group.}
\label{stochastic table}
\end{table}
\FloatBarrier

\vspace{5mm}
\noindent
The stochastic name used within the stochastic table is generated by using the predecessor module name in the production rule, as well as the number of parameters within the predecessor module. In the example above, we can use the predecessor name F, which has a single parameter, making the stochastic name F1. This method of naming serves as a unique identifier for the stochastic group. Once all of the production rules are processed, each groups' probabilities are added together. The total probability should equal 1.0. A tolerance should put in place to account for floating-point error.

During the rewriting process, the module that is being rewritten is matched to a particular stochastic group. A uniformly distributed random number is generated between 0.0 and 1.0. A range for each rule is generated, for instance, p1 will be between 0.0 and 0.33, p2 will be between 0.33 and 0.66, and finally, p3 will be between 0.66 and 1.0. The production rule will be chosen where the random number falls between. For example, if the random number is 0.456, p2 will be chosen as 0.456 falls between 0.33 and 0.66.

\section{The String Rewriter}

Once the L-system has been processed by the lexical analyser and the parser, the L-systems' resulting data structures are ready for string rewriting. All of the data structures necessary for rewriting can be seen in the list below. A definition of these data structures can be seen in appendix \ref{rewriter data structures}.

\begin{singlespace}
\begin{itemize}
	\item Constant variables table
	\item Local variable table
	\item Number of generations
	\item Production rules
	\begin{itemize}
		\item Predecessor module
		\item Condition or stochastic probability
		\item Successor string of modules
	\end{itemize}
	\item Axiom string of modules
\end{itemize}
\end{singlespace}

\newpage
\noindent
The string rewriter is the final stage in the rewriter system. It starts with the axiom as the current string of modules. It then iterates over each module within the current string, matching it to the production rules. If the module matches a rule, the modules' parameter values are matched to the predecessors' parameter variable names and stored in the local variable table. The variables in the production rules successor are replaced according to the constant and local variable tables and subsequently evaluated. The production rule successor is then stored in a result string. If a match is not found, the module itself is stored in the result string. Once all the modules have been rewritten, the result string replaces the current string, and the local table is emptied. This process is carried out for the number of generations specified by the L-system and will eventually provide the final result string of modules.

\begin{figure}[htbp]
	{\centering
		\setlength{\fboxrule}{1pt}
		\vspace{7px}
		\fbox{
			\includegraphics[scale=0.5]{Diagrams/rewriter.png}
		}
		\caption{Simplified flow chart of string rewriting procedure.} \label{rewriter diagram}
	}
\end{figure}
\FloatBarrier

\noindent
The rewriting procedure can be summarised in the flow chart above for a more in depth description of the procedure the pseudocode, as well as several useful functions, can be found in appendix \ref{rewriter pseudocode}. 

\newpage
\section{Summary}

The L-system rewriter is the first of two major systems within the process of procedurally generating plant-life. The second system is the interpreter. The rewriters' purpose is to take an L-system input and understand its grammatical structure and carry out string rewriting.

The rewriter system acts as a type of compiler, similar to a computer language. The L-system becomes a type of language that the L-system rewriter can understand. This understanding allows the creation of data structures that are used during the rewriting process. There are two stages that process the L-system input; these are the lexical analyser and the parser. These stages will give informative messages if there is a mistake, either grammatically or syntactically. If the language meets all of the grammatical and syntactic requirements, the relevant data structures are created, and the string rewriter can use this to generate the resultant string of modules. The result string produced by the rewriting system will always be a valid string according to the L-system grammar. 

The L-system rewriter can be used for many different applications and is not limited to that of procedural plant generation. The interpreter uses the resultant string to create the final rendered representation, such as the plant model. The advantage of having the rewriter be complex is that the rewriting system does not need to change, even if the L-system is used for a different purpose.  Only the interpretation will need to change in order to understand the resulting string. This is the main reason behind using a compiler-like process to govern the string rewriting. It allows the L-system enough complexity to provide information to the interpreter, but not so much that interpretation becomes reliant on the string rewriter.